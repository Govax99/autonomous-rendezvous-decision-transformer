{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collator definition\n",
    "\n",
    "A collator is a helper class for the training loop. It is responsible for batching together the individual samples and preparing them for the model. The collator is called by the DataLoader for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 20  # size of state space\n",
    "    act_dim: int = 6  # size of action space\n",
    "    max_ep_len: int = 1000 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "        self.dataset = dataset\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"observations\"]:\n",
    "            states.extend(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        self.n_traj = len(traj_lens)\n",
    "        states = np.vstack(states)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        \n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        return {\n",
    "            \n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define trainable transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = super().forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        \n",
    "        loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset\n",
    "\n",
    "Dataset defined as dictionary with train, validation and test sets. Each set is a list of dictionaries with the following keys:\n",
    "- observations: [dim_obs x episode_length] numpy array\n",
    "- actions: [dim_act x episode_length] numpy array\n",
    "- rewards: [episode_length] numpy array\n",
    "- dones: [episode_length] numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_parquet('decision_transformer_satellites_rendezvous-train.parquet')\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "dataset = DatasetDict({\"train\":train_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['observations', 'actions', 'rewards', 'dones'],\n",
       "        num_rows: 42\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
    "\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDT(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05e6a1af4d34ebe85dd457052ee362c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 19.2511, 'train_samples_per_second': 261.803, 'train_steps_per_second': 6.233, 'train_loss': 0.010625876983006795, 'epoch': 120.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=120,\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"output/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop environment for prediction loop\n",
    "\n",
    "We use gym library and create a custom environment for the prediction loop. The environment is defined as a class with the following methods:\n",
    "- reset: resets the environment and returns the initial observation\n",
    "- step: takes an action and returns the next observation, reward, done flag and info dictionary\n",
    "- render: renders the environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should define limits similar to what is done for matlab\n",
    "from dataclasses import dataclass\n",
    "# ----- PARAMETERS FOR DYNAMIC SIMULATION ----- %\n",
    "\n",
    "@dataclass\n",
    "class parameters:         \n",
    "    LC : float = 1\n",
    "    LT : float = 3\n",
    "    J_C : np.ndarray = np.eye(3)\n",
    "    J_T : np.ndarray = np.eye(3)\n",
    "    m_C : float = 1\n",
    "    OM : float = 0.005\n",
    "    pberth : np.ndarray = np.array([5, 0, 0])\n",
    "    kP_tr : np.ndarray = 0.1*np.eye(3)\n",
    "    kD_tr : np.ndarray = 1*np.eye(3)\n",
    "    kP_rot : np.ndarray = 1*np.eye(3)\n",
    "    u_lim : np.ndarray = np.ones((6,))\n",
    "    r2 : float = 3**2 # keep out zone\n",
    "    timestep : float = 0.3\n",
    "\n",
    "\n",
    "\n",
    "# ----- META-PARAMETERS: OPTIONS FOR SOLVERS AND REWARD DEFINITION ----- %\n",
    "@dataclass\n",
    "class options:\n",
    "    K_action : np.ndarray = np.eye(6)\n",
    "    R_success : float = 5\n",
    "    R_collision : float = -10\n",
    "    R_timeout : float = -5\n",
    "\n",
    "    pos_low_lim : np.ndarray = np.array([-15, -15, -15])\n",
    "    pos_high_lim : np.ndarray = np.array([15, 15, 15])\n",
    "    vel_low_lim : np.ndarray = np.array([-0.1, -0.1, -0.1])\n",
    "    vel_high_lim : np.ndarray = np.array([0.1, 0.1, 0.1])\n",
    "    ang_low_lim : np.ndarray = np.array([-0.1, -0.1, -0.1])\n",
    "    ang_high_lim : np.ndarray = np.array([0.1, 0.1, 0.1])\n",
    "    quat_low_lim : np.ndarray = np.array([-1, -1, -1, -1])\n",
    "    quat_high_lim : np.ndarray = np.array([1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env, spaces\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import quat\n",
    "import ode_model\n",
    "# quaternions\n",
    "# other\n",
    "\n",
    "def random_state():\n",
    "    pos = np.random.uniform(size=(3,), low=options.pos_low_lim, high=options.pos_high_lim)\n",
    "    vel = np.random.uniform(size=(3,), low=options.vel_low_lim, high=options.vel_high_lim)\n",
    "    quat_chaser = np.random.uniform(size=(4,), low=options.quat_low_lim, high=options.quat_high_lim)\n",
    "    quat_chaser = quat_chaser / np.linalg.norm(quat_chaser)\n",
    "    ang_vel_chaser = np.random.uniform(size=(3,), low=options.ang_low_lim, high=options.ang_high_lim)\n",
    "    quat_target = np.random.uniform(size=(4,), low=options.quat_low_lim, high=options.quat_high_lim)\n",
    "    quat_target = quat_target / np.linalg.norm(quat_target)\n",
    "    ang_vel_target = np.random.uniform(size=(3,), low=options.ang_low_lim, high=options.ang_high_lim)\n",
    "    return np.concatenate((pos, vel, quat_chaser, ang_vel_chaser, quat_target, ang_vel_target))\n",
    "\n",
    "def check_success(state):\n",
    "    p_LC_L = state[0:3]\n",
    "    v_LC_L = state[3:6]\n",
    "    q_LC = state[6:10]/np.linalg.norm(state[6:10])\n",
    "    w_IC_C = state[10:13]\n",
    "    q_LT = state[13:17]/np.linalg.norm(state[13:17])\n",
    "    w_IT_T = state[17:20]\n",
    "\n",
    "    OM = parameters.OM\n",
    "    OM_IL_L = np.array([0, 0, OM])\n",
    "\n",
    "    p_LC_L_check = quat.rotate(parameters.pberth, q_LT)\n",
    "    R_LC = quat.quat2rotm(q_LC)\n",
    "    w_LC_L = R_LC @ w_IC_C - OM_IL_L # ang. velocity of line of sight chaser-target\n",
    "    v_LC_L_check = np.cross(w_LC_L, p_LC_L_check) # chaser must have this velocity to keep up with rotation\n",
    "    err = np.linalg.norm(p_LC_L - p_LC_L_check) + np.linalg.norm(v_LC_L - v_LC_L_check) + np.linalg.norm(q_LC - q_LT) + np.linalg.norm(w_IC_C - w_IT_T)\n",
    "    \n",
    "    tol = 1e-6\n",
    "    if err < tol:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def compute_reward(obs, action):\n",
    "    reward = parameters.timestep*np.linalg.norm(options.K_action @ action)\n",
    "\n",
    "    if np.linalg.norm(obs[0:3]) < np.sqrt(parameters.r2):\n",
    "        reward += options.R_collision\n",
    "\n",
    "    if check_success(obs):\n",
    "        reward += options.R_success\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "class SpacecraftRendezvous(Env):\n",
    "    def __init__(self):\n",
    "        super(SpacecraftRendezvous, self).__init__()\n",
    "        \n",
    "        # Define observation space\n",
    "        self.observation_shape = (20,)\n",
    "        self.observation_space = spaces.Box(low = np.full(self.observation_shape, -np.inf), \n",
    "                                            high = np.full(self.observation_shape, np.inf),\n",
    "                                            dtype = np.float64)\n",
    "    \n",
    "        \n",
    "        # Define an action space \n",
    "        self.action_shape = (6,)\n",
    "        self.action_space = spaces.Box(low = -parameters.u_lim, \n",
    "                                            high = parameters.u_lim,\n",
    "                                            dtype = np.float64)\n",
    "        \n",
    "        self.timestep = 0.3\n",
    "        self.current_state = random_state()\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.current_state = random_state()\n",
    "        while np.linalg.norm(self.current_state[0:3]) < np.sqrt(parameters.r2):\n",
    "            self.current_state = random_state()\n",
    "\n",
    "        observation = self.current_state\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action): # TODO : use a certain dt instead of hardcoded from definition\n",
    "        # propagate the dynamics\n",
    "        dt = parameters.timestep\n",
    "        o = self.current_state\n",
    "        sol = solve_ivp(lambda t, state : ode_model.dynamics(t, state, parameters, action), [0, dt], o, atol=1e-6, rtol=1e-6)\n",
    "        observation = sol.y[:,-1]\n",
    "        self.current_state = observation\n",
    "\n",
    "\n",
    "        # check for impact and if reached objective\n",
    "        if np.linalg.norm(self.current_state[0:3]) < np.sqrt(parameters.r2):\n",
    "            done = True\n",
    "        elif check_success(observation):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # compute the reward\n",
    "        reward = compute_reward(observation, action)\n",
    "\n",
    "        return observation, reward, done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables for prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.1382589e+00 -1.3246049e+00  4.6059570e-01  4.3193098e-02\n",
      "  2.7685380e-02 -1.8177604e-02  2.4311738e-02  1.7281880e-03\n",
      " -1.6144600e-02  8.6094147e-01 -2.1195339e-02 -6.4675897e-02\n",
      " -9.2611000e-02 -1.0331314e-02  1.3281289e-03 -4.3708045e-02\n",
      "  4.7704020e-01 -5.1562369e-02  2.7764983e-02  6.1281215e-02]\n"
     ]
    }
   ],
   "source": [
    "# build the environment\n",
    "model = model.to(\"cpu\")\n",
    "env = SpacecraftRendezvous()\n",
    "max_ep_len = 1000\n",
    "device = \"cpu\"\n",
    "scale = 100.0  # normalization for rewards/returns\n",
    "TARGET_RETURN = 95 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
    "\n",
    "state_mean = collator.state_mean.astype(np.float32)\n",
    "state_std = collator.state_std.astype(np.float32)\n",
    "print(state_mean)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Create the decision transformer model\n",
    "\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that compute predicted action from auto-regressive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gets an action from the model using autoregressive prediction with a window of the previous 20 timesteps.\n",
    "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "    # This implementation does not condition on past rewards\n",
    "\n",
    "    states = states.reshape(1, -1, model.config.state_dim)\n",
    "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "    timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "    states = states[:, -model.config.max_length :]\n",
    "    actions = actions[:, -model.config.max_length :]\n",
    "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "    timesteps = timesteps[:, -model.config.max_length :]\n",
    "    padding = model.config.max_length - states.shape[1]\n",
    "    # pad all tokens to sequence length\n",
    "    attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
    "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
    "    returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "    state_preds, action_preds, return_preds = model.original_forward(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        returns_to_go=returns_to_go,\n",
    "        timesteps=timesteps,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    return action_preds[0, -1]\n",
    "\n",
    "\n",
    "def saturate(action):\n",
    "    lim = 1\n",
    "    for i in range(len(action)):\n",
    "        if (action[i] > lim):\n",
    "            action[i] = lim\n",
    "        elif (action[i] < -lim):\n",
    "            action[i] = -lim\n",
    "\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interact with the environment and create a video\n",
    "episode_return, episode_length = 0, 0\n",
    "state = env.reset()\n",
    "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "for t in range(max_ep_len):\n",
    "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "    action = get_action(\n",
    "        model,\n",
    "        (states - state_mean) / state_std,\n",
    "        actions,\n",
    "        rewards,\n",
    "        target_return,\n",
    "        timesteps,\n",
    "    )\n",
    "    actions[-1] = action\n",
    "    action = action.detach().cpu().numpy()\n",
    "    action = action.reshape((-1,))\n",
    "    # action limitation\n",
    "    action = saturate(action)\n",
    "    state, reward, done = env.step(action)\n",
    "\n",
    "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "    states = torch.cat([states, cur_state], dim=0)\n",
    "    rewards[-1] = reward\n",
    "\n",
    "    pred_return = target_return[0, -1] - (reward / scale)\n",
    "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "    episode_return += reward\n",
    "    episode_length += 1\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export results to matlab for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "mdic = {\"x2\": states.detach().numpy()}\n",
    "savemat(r\"..\\optimal-control\\result_vis.mat\", mdic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
