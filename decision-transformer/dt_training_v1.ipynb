{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collator definition\n",
    "\n",
    "A collator is a helper class for the training loop. It is responsible for batching together the individual samples and preparing them for the model. The collator is called by the DataLoader for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 20  # size of state space\n",
    "    act_dim: int = 6  # size of action space\n",
    "    max_ep_len: int = 1000 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "        self.dataset = dataset\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"observations\"]:\n",
    "            states.extend(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        self.n_traj = len(traj_lens)\n",
    "        states = np.vstack(states)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        \n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        return {\n",
    "            \n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define trainable transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = super().forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        \n",
    "        loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset\n",
    "\n",
    "Dataset defined as dictionary with train, validation and test sets. Each set is a list of dictionaries with the following keys:\n",
    "- observations: [dim_obs x episode_length] numpy array\n",
    "- actions: [dim_act x episode_length] numpy array\n",
    "- rewards: [episode_length] numpy array\n",
    "- dones: [episode_length] numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_parquet('decision_transformer_satellites_rendezvous-train.parquet')\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "dataset = DatasetDict({\"train\":train_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['observations', 'actions', 'rewards', 'dones'],\n",
       "        num_rows: 42\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
    "\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDT(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d9633d3ff3483b92157ebf7f94e360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0043, 'learning_rate': 8.333333333333334e-06, 'epoch': 1.0}\n",
      "{'loss': 0.0022, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0026, 'learning_rate': 2.5e-05, 'epoch': 3.0}\n",
      "{'loss': 0.0026, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0022, 'learning_rate': 4.166666666666667e-05, 'epoch': 5.0}\n",
      "{'loss': 0.0028, 'learning_rate': 5e-05, 'epoch': 6.0}\n",
      "{'loss': 0.0024, 'learning_rate': 5.833333333333334e-05, 'epoch': 7.0}\n",
      "{'loss': 0.0022, 'learning_rate': 6.666666666666667e-05, 'epoch': 8.0}\n",
      "{'loss': 0.0021, 'learning_rate': 7.500000000000001e-05, 'epoch': 9.0}\n",
      "{'loss': 0.0023, 'learning_rate': 8.333333333333334e-05, 'epoch': 10.0}\n",
      "{'loss': 0.0023, 'learning_rate': 9.166666666666667e-05, 'epoch': 11.0}\n",
      "{'loss': 0.0021, 'learning_rate': 0.0001, 'epoch': 12.0}\n",
      "{'loss': 0.002, 'learning_rate': 9.907407407407407e-05, 'epoch': 13.0}\n",
      "{'loss': 0.0021, 'learning_rate': 9.814814814814815e-05, 'epoch': 14.0}\n",
      "{'loss': 0.002, 'learning_rate': 9.722222222222223e-05, 'epoch': 15.0}\n",
      "{'loss': 0.0018, 'learning_rate': 9.62962962962963e-05, 'epoch': 16.0}\n",
      "{'loss': 0.002, 'learning_rate': 9.537037037037038e-05, 'epoch': 17.0}\n",
      "{'loss': 0.002, 'learning_rate': 9.444444444444444e-05, 'epoch': 18.0}\n",
      "{'loss': 0.0019, 'learning_rate': 9.351851851851852e-05, 'epoch': 19.0}\n",
      "{'loss': 0.0017, 'learning_rate': 9.25925925925926e-05, 'epoch': 20.0}\n",
      "{'loss': 0.002, 'learning_rate': 9.166666666666667e-05, 'epoch': 21.0}\n",
      "{'loss': 0.0029, 'learning_rate': 9.074074074074075e-05, 'epoch': 22.0}\n",
      "{'loss': 0.0019, 'learning_rate': 8.981481481481481e-05, 'epoch': 23.0}\n",
      "{'loss': 0.0017, 'learning_rate': 8.888888888888889e-05, 'epoch': 24.0}\n",
      "{'loss': 0.002, 'learning_rate': 8.796296296296297e-05, 'epoch': 25.0}\n",
      "{'loss': 0.0032, 'learning_rate': 8.703703703703704e-05, 'epoch': 26.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moutput/\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     remove_unused_columns\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     logging_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     15\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     16\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     17\u001b[0m     train_dataset\u001b[39m=\u001b[39mdataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     18\u001b[0m     data_collator\u001b[39m=\u001b[39mcollator,\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 21\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     22\u001b[0m train_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mlog_history)\n\u001b[0;32m     23\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m\"\u001b[39m\u001b[39moutput/\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Davide\\Desktop\\thesis\\implementation\\autonomous-rendezvous-decision-transformer\\decision-transformer\\env\\lib\\site-packages\\transformers\\trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1644\u001b[0m )\n\u001b[1;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1650\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Davide\\Desktop\\thesis\\implementation\\autonomous-rendezvous-decision-transformer\\decision-transformer\\env\\lib\\site-packages\\transformers\\trainer.py:1916\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1913\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1915\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1916\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1917\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1918\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mc:\\Users\\Davide\\Desktop\\thesis\\implementation\\autonomous-rendezvous-decision-transformer\\decision-transformer\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Davide\\Desktop\\thesis\\implementation\\autonomous-rendezvous-decision-transformer\\decision-transformer\\env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Davide\\Desktop\\thesis\\implementation\\autonomous-rendezvous-decision-transformer\\decision-transformer\\env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m, in \u001b[0;36mDecisionTransformerGymDataCollator.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     47\u001b[0m s, a, r, d, rtg, timesteps, mask \u001b[39m=\u001b[39m [], [], [], [], [], [], []\n\u001b[0;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m batch_inds:\n\u001b[0;32m     50\u001b[0m     \u001b[39m# for feature in features:\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mint\u001b[39;49m(ind)]\n\u001b[0;32m     52\u001b[0m     si \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(feature[\u001b[39m\"\u001b[39m\u001b[39mrewards\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m     \u001b[39m# get sequences from dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Davide\\Desktop\\thesis\\implementation\\autonomous-rendezvous-decision-transformer\\decision-transformer\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:2792\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2790\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2791\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2792\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[1;32mc:\\Users\\Davide\\Desktop\\thesis\\implementation\\autonomous-rendezvous-decision-transformer\\decision-transformer\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:2776\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2774\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   2775\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2776\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   2777\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[0;32m   2778\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[0;32m   2779\u001b[0m )\n\u001b[0;32m   2780\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\Davide\\Desktop\\thesis\\implementation\\autonomous-rendezvous-decision-transformer\\decision-transformer\\env\\lib\\site-packages\\datasets\\formatting\\formatting.py:586\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[39m# Query the main table\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 586\u001b[0m     pa_subtable \u001b[39m=\u001b[39m _query_table(table, key)\n\u001b[0;32m    587\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    588\u001b[0m     pa_subtable \u001b[39m=\u001b[39m _query_table_with_indices_mapping(table, key, indices\u001b[39m=\u001b[39mindices)\n",
      "File \u001b[1;32mc:\\Users\\Davide\\Desktop\\thesis\\implementation\\autonomous-rendezvous-decision-transformer\\decision-transformer\\env\\lib\\site-packages\\datasets\\formatting\\formatting.py:85\u001b[0m, in \u001b[0;36m_query_table\u001b[1;34m(table, key)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39mQuery a pyarrow Table to extract the subtable that correspond to the given key.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mint\u001b[39m):\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39;49mfast_slice(key \u001b[39m%\u001b[39;49m table\u001b[39m.\u001b[39;49mnum_rows, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n\u001b[0;32m     87\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39mkey\u001b[39m.\u001b[39mindices(table\u001b[39m.\u001b[39mnum_rows))\n",
      "File \u001b[1;32mc:\\Users\\Davide\\Desktop\\thesis\\implementation\\autonomous-rendezvous-decision-transformer\\decision-transformer\\env\\lib\\site-packages\\datasets\\table.py:162\u001b[0m, in \u001b[0;36mIndexedTableMixin.fast_slice\u001b[1;34m(self, offset, length)\u001b[0m\n\u001b[0;32m    160\u001b[0m     batches[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m batches[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mslice(\u001b[39m0\u001b[39m, offset \u001b[39m+\u001b[39m length \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offsets[j])\n\u001b[0;32m    161\u001b[0m     batches[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m batches[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mslice(offset \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offsets[i])\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_batches(batches, schema\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_schema)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=120,\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "train_data = pd.DataFrame(trainer.state.log_history)\n",
    "model.save_pretrained(\"output/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop environment for prediction loop\n",
    "\n",
    "We use gym library and create a custom environment for the prediction loop. The environment is defined as a class with the following methods:\n",
    "- reset: resets the environment and returns the initial observation\n",
    "- step: takes an action and returns the next observation, reward, done flag and info dictionary\n",
    "- render: renders the environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should define limits similar to what is done for matlab\n",
    "from dataclasses import dataclass\n",
    "# ----- PARAMETERS FOR DYNAMIC SIMULATION ----- %\n",
    "\n",
    "@dataclass\n",
    "class parameters:         \n",
    "    LC : float = 1\n",
    "    LT : float = 3\n",
    "    J_C : np.ndarray = np.eye(3)\n",
    "    J_T : np.ndarray = np.eye(3)\n",
    "    m_C : float = 1\n",
    "    OM : float = 0.005\n",
    "    pberth : np.ndarray = np.array([5, 0, 0])\n",
    "    kP_tr : np.ndarray = 0.1*np.eye(3)\n",
    "    kD_tr : np.ndarray = 1*np.eye(3)\n",
    "    kP_rot : np.ndarray = 1*np.eye(3)\n",
    "    u_lim : np.ndarray = np.ones((6,))\n",
    "    r2 : float = 3**2 # keep out zone\n",
    "    timestep : float = 0.3\n",
    "\n",
    "\n",
    "\n",
    "# ----- META-PARAMETERS: OPTIONS FOR SOLVERS AND REWARD DEFINITION ----- %\n",
    "@dataclass\n",
    "class options:\n",
    "    K_action : np.ndarray = np.eye(6)\n",
    "    R_success : float = 5\n",
    "    R_collision : float = -10\n",
    "    R_timeout : float = -5\n",
    "\n",
    "    pos_low_lim : np.ndarray = np.array([-15, -15, -15])\n",
    "    pos_high_lim : np.ndarray = np.array([15, 15, 15])\n",
    "    vel_low_lim : np.ndarray = np.array([-0.1, -0.1, -0.1])\n",
    "    vel_high_lim : np.ndarray = np.array([0.1, 0.1, 0.1])\n",
    "    ang_low_lim : np.ndarray = np.array([-0.1, -0.1, -0.1])\n",
    "    ang_high_lim : np.ndarray = np.array([0.1, 0.1, 0.1])\n",
    "    quat_low_lim : np.ndarray = np.array([-1, -1, -1, -1])\n",
    "    quat_high_lim : np.ndarray = np.array([1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env, spaces\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import quat\n",
    "import ode_model\n",
    "# quaternions\n",
    "# other\n",
    "\n",
    "def random_state():\n",
    "    pos = np.random.uniform(size=(3,), low=options.pos_low_lim, high=options.pos_high_lim)\n",
    "    vel = np.random.uniform(size=(3,), low=options.vel_low_lim, high=options.vel_high_lim)\n",
    "    quat_chaser = np.random.uniform(size=(4,), low=options.quat_low_lim, high=options.quat_high_lim)\n",
    "    quat_chaser = quat_chaser / np.linalg.norm(quat_chaser)\n",
    "    ang_vel_chaser = np.random.uniform(size=(3,), low=options.ang_low_lim, high=options.ang_high_lim)\n",
    "    quat_target = np.random.uniform(size=(4,), low=options.quat_low_lim, high=options.quat_high_lim)\n",
    "    quat_target = quat_target / np.linalg.norm(quat_target)\n",
    "    ang_vel_target = np.random.uniform(size=(3,), low=options.ang_low_lim, high=options.ang_high_lim)\n",
    "    return np.concatenate((pos, vel, quat_chaser, ang_vel_chaser, quat_target, ang_vel_target))\n",
    "\n",
    "def check_success(state):\n",
    "    p_LC_L = state[0:3]\n",
    "    v_LC_L = state[3:6]\n",
    "    q_LC = state[6:10]/np.linalg.norm(state[6:10])\n",
    "    w_IC_C = state[10:13]\n",
    "    q_LT = state[13:17]/np.linalg.norm(state[13:17])\n",
    "    w_IT_T = state[17:20]\n",
    "\n",
    "    OM = parameters.OM\n",
    "    OM_IL_L = np.array([0, 0, OM])\n",
    "\n",
    "    p_LC_L_check = quat.rotate(parameters.pberth, q_LT)\n",
    "    R_LC = quat.quat2rotm(q_LC)\n",
    "    w_LC_L = R_LC @ w_IC_C - OM_IL_L # ang. velocity of line of sight chaser-target\n",
    "    v_LC_L_check = np.cross(w_LC_L, p_LC_L_check) # chaser must have this velocity to keep up with rotation\n",
    "    err = np.linalg.norm(p_LC_L - p_LC_L_check) + np.linalg.norm(v_LC_L - v_LC_L_check) + np.linalg.norm(q_LC - q_LT) + np.linalg.norm(w_IC_C - w_IT_T)\n",
    "    \n",
    "    tol = 1e-6\n",
    "    if err < tol:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def compute_reward(obs, action):\n",
    "    reward = parameters.timestep*np.linalg.norm(options.K_action @ action)\n",
    "\n",
    "    if np.linalg.norm(obs[0:3]) < np.sqrt(parameters.r2):\n",
    "        reward += options.R_collision\n",
    "\n",
    "    if check_success(obs):\n",
    "        reward += options.R_success\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "class SpacecraftRendezvous(Env):\n",
    "    def __init__(self):\n",
    "        super(SpacecraftRendezvous, self).__init__()\n",
    "        \n",
    "        # Define observation space\n",
    "        self.observation_shape = (20,)\n",
    "        self.observation_space = spaces.Box(low = np.full(self.observation_shape, -np.inf), \n",
    "                                            high = np.full(self.observation_shape, np.inf),\n",
    "                                            dtype = np.float64)\n",
    "    \n",
    "        \n",
    "        # Define an action space \n",
    "        self.action_shape = (6,)\n",
    "        self.action_space = spaces.Box(low = -parameters.u_lim, \n",
    "                                            high = parameters.u_lim,\n",
    "                                            dtype = np.float64)\n",
    "        \n",
    "        self.timestep = 0.3\n",
    "        self.current_state = random_state()\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.current_state = random_state()\n",
    "        while np.linalg.norm(self.current_state[0:3]) < np.sqrt(parameters.r2):\n",
    "            self.current_state = random_state()\n",
    "\n",
    "        observation = self.current_state\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def step(self, action): # TODO : use a certain dt instead of hardcoded from definition\n",
    "        # propagate the dynamics\n",
    "        dt = parameters.timestep\n",
    "        o = self.current_state\n",
    "        sol = solve_ivp(lambda t, state : ode_model.dynamics(t, state, parameters, action), [0, dt], o, atol=1e-6, rtol=1e-6)\n",
    "        observation = sol.y[:,-1]\n",
    "        self.current_state = observation\n",
    "\n",
    "\n",
    "        # check for impact and if reached objective\n",
    "        if np.linalg.norm(self.current_state[0:3]) < np.sqrt(parameters.r2):\n",
    "            done = True\n",
    "        elif check_success(observation):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # compute the reward\n",
    "        reward = compute_reward(observation, action)\n",
    "\n",
    "        return observation, reward, done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables for prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.1382589e+00 -1.3246049e+00  4.6059570e-01  4.3193098e-02\n",
      "  2.7685380e-02 -1.8177604e-02  2.4311738e-02  1.7281880e-03\n",
      " -1.6144600e-02  8.6094147e-01 -2.1195339e-02 -6.4675897e-02\n",
      " -9.2611000e-02 -1.0331314e-02  1.3281289e-03 -4.3708045e-02\n",
      "  4.7704020e-01 -5.1562369e-02  2.7764983e-02  6.1281215e-02]\n"
     ]
    }
   ],
   "source": [
    "# build the environment\n",
    "model = model.to(\"cpu\")\n",
    "env = SpacecraftRendezvous()\n",
    "max_ep_len = 1000\n",
    "device = \"cpu\"\n",
    "scale = 100.0  # normalization for rewards/returns\n",
    "TARGET_RETURN = 95 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
    "\n",
    "state_mean = collator.state_mean.astype(np.float32)\n",
    "state_std = collator.state_std.astype(np.float32)\n",
    "print(state_mean)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Create the decision transformer model\n",
    "\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that compute predicted action from auto-regressive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gets an action from the model using autoregressive prediction with a window of the previous 20 timesteps.\n",
    "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "    # This implementation does not condition on past rewards\n",
    "\n",
    "    states = states.reshape(1, -1, model.config.state_dim)\n",
    "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "    timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "    states = states[:, -model.config.max_length :]\n",
    "    actions = actions[:, -model.config.max_length :]\n",
    "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "    timesteps = timesteps[:, -model.config.max_length :]\n",
    "    padding = model.config.max_length - states.shape[1]\n",
    "    # pad all tokens to sequence length\n",
    "    attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
    "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
    "    returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "    state_preds, action_preds, return_preds = model.original_forward(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        returns_to_go=returns_to_go,\n",
    "        timesteps=timesteps,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    return action_preds[0, -1]\n",
    "\n",
    "\n",
    "def saturate(action):\n",
    "    lim = 1\n",
    "    for i in range(len(action)):\n",
    "        if (action[i] > lim):\n",
    "            action[i] = lim\n",
    "        elif (action[i] < -lim):\n",
    "            action[i] = -lim\n",
    "\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interact with the environment and create a video\n",
    "episode_return, episode_length = 0, 0\n",
    "state = env.reset()\n",
    "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "for t in range(max_ep_len):\n",
    "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "    action = get_action(\n",
    "        model,\n",
    "        (states - state_mean) / state_std,\n",
    "        actions,\n",
    "        rewards,\n",
    "        target_return,\n",
    "        timesteps,\n",
    "    )\n",
    "    actions[-1] = action\n",
    "    action = action.detach().cpu().numpy()\n",
    "    action = action.reshape((-1,))\n",
    "    # action limitation\n",
    "    action = saturate(action)\n",
    "    state, reward, done = env.step(action)\n",
    "\n",
    "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "    states = torch.cat([states, cur_state], dim=0)\n",
    "    rewards[-1] = reward\n",
    "\n",
    "    pred_return = target_return[0, -1] - (reward / scale)\n",
    "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "    episode_return += reward\n",
    "    episode_length += 1\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export results to matlab for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "mdic = {\"x2\": states.detach().numpy()}\n",
    "savemat(r\"..\\optimal-control\\result_vis.mat\", mdic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
